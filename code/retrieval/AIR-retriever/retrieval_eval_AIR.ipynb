{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "retrieval_eval_AIR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka-56Dhn-Pj2"
      },
      "source": [
        "!pip install semantic-text-similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq4jTmJP-YCc"
      },
      "source": [
        "from semantic_text_similarity.models import WebBertSimilarity\n",
        "from semantic_text_similarity.models import ClinicalBertSimilarity\n",
        "\n",
        "web_model = WebBertSimilarity(device='cpu', batch_size=10) #defaults to GPU prediction\n",
        "\n",
        "# clinical_model = ClinicalBertSimilarity(device='cuda', batch_size=10) #defaults to GPU prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ViXBvEA-Znn"
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import json\n",
        "class GFG: \n",
        "  def __init__(self,graph): \n",
        "    self.graph = graph \n",
        "    self.ppl = len(graph) \n",
        "    self.jobs = len(graph[0]) \n",
        "\n",
        "  def bpm(self, u, matchR, seen): \n",
        "    for v in range(self.jobs):  \n",
        "      if self.graph[u][v] and seen[v] == False: \n",
        "        seen[v] = True\n",
        "        if matchR[v] == -1 or self.bpm(matchR[v], matchR, seen): \n",
        "          matchR[v] = u \n",
        "          return True\n",
        "    return False\n",
        "  \n",
        "  # Returns maximum number of matching \n",
        "  def maxBPM(self):\n",
        "    matchR = [-1] * self.jobs \n",
        "\n",
        "    result = 0\n",
        "    for i in range(self.ppl):\n",
        "      seen = [False] * self.jobs\n",
        "      if self.bpm(i, matchR, seen): \n",
        "        result += 1\n",
        "    return result, matchR\n",
        "\n",
        "def my_lcs(string, sub):\n",
        "  if(len(string)<= len(sub)):\n",
        "    sub, string = string, sub\n",
        "\n",
        "    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n",
        "\n",
        "    for j in range(1,len(sub)+1):\n",
        "      for i in range(1,len(string)+1):\n",
        "        if (string[i-1] == sub[j-1]):\n",
        "          lengths[i][j] = lengths[i-1][j-1] + 1\n",
        "        else:\n",
        "          lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n",
        "\n",
        "    return lengths[len(string)][len(sub)]\n",
        "\n",
        "class Rouge():\n",
        "  def __init__(self):\n",
        "    self.beta = 1.2\n",
        "\n",
        "  def calc_score(self, candidate, refs):\n",
        "    assert(len(candidate)==1)\t\n",
        "    assert(len(refs)>0)         \n",
        "    prec = []\n",
        "    rec = []\n",
        "\n",
        "  # split into tokens\n",
        "    token_c = candidate[0].split(\" \")\n",
        "    \t\n",
        "    for reference in refs:\n",
        "      # split into tokens\n",
        "      token_r = reference.split(\" \")\n",
        "      # compute the longest common subsequence\n",
        "      lcs = my_lcs(token_r, token_c)\n",
        "      if (lcs == None):\n",
        "        prec.append(0)\n",
        "        rec.append(0)\n",
        "      else:\n",
        "        prec.append(lcs/float(len(token_c)))\n",
        "        rec.append(lcs/float(len(token_r)))\n",
        "\n",
        "      prec_max = max(prec)\n",
        "      rec_max = max(rec)\n",
        "\n",
        "      if (prec_max!=0 and rec_max !=0):\n",
        "        score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n",
        "      else:\n",
        "        score = 0.0\n",
        "      return score\n",
        "\n",
        "  def compute_score(self, refs, test):\n",
        "    score = []\n",
        "    for i in range(len(refs)):\n",
        "      hypo = test[i]\n",
        "      ref = refs[i]\n",
        "      if (hypo == \" \" or hypo == \"\"):\n",
        "        score.append(0)\n",
        "      else:\n",
        "        score.append(self.calc_score([hypo], [ref]))\n",
        "    \n",
        "    average_score = np.mean(np.array(score))\n",
        "    return average_score, np.array(score)\n",
        "\n",
        "  def method(self):\n",
        "    return \"Rouge\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI0qGPHu-bXW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLODQSZt-dAe"
      },
      "source": [
        "For evaluation of AIR approach on gold corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIzX5wEv-gI4"
      },
      "source": [
        "output_file = './MultiRC_BM25_vs_POCC_justification_quality_score/air_test_output.tsv'\n",
        "test_file = '../data/E2_test.json'\n",
        "name = 'air_test_output'\n",
        "f = open(test_file)\n",
        "data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vOZF8NR_AN2"
      },
      "source": [
        "output_cols = [\"start\", \"index\", \"zero_zero\", \"correctness\", \"ques\", \"prop0\", \"prop1\", \"prop2\", \"prop3\", \"prop4\", \"prop5\", \"prop6\", \"prop7\",\"prop8\",\"prop9\",\"prop10\",\"prop11\",\"prop12\",\"prop13\",\"prop14\",\"prop15\",\"prop16\",\"prop17\",\"prop18\",\"prop19\",\"prop20\"]\n",
        "df = pd.read_csv(output_file, sep='\\t', names=output_cols, engine='python', header=None)\n",
        "df = df.fillna(\"\")\n",
        "\n",
        "output_queries = df['ques']\n",
        "output_props = []\n",
        "output_correctness = df['correctness']\n",
        "\n",
        "for i,row in df.iterrows():\n",
        "  props = [row['prop0'],row['prop1'],row['prop2'],row['prop3'],row['prop4'],row['prop5'],row['prop6'],row['prop7'],row['prop8'],row['prop9'],row['prop10'],row['prop11'],row['prop12'],row['prop13'],row['prop14'],row['prop15'],row['prop16'],row['prop17'],row['prop18'],row['prop19'],row['prop20']]\n",
        "  output_props.append(props)\n",
        "\n",
        "count = 0\n",
        "recall = 0.0\n",
        "precision = 0.0\n",
        "f_score = 0.0\n",
        "for i in range(0, len(data['q_text'])):\n",
        "  # if i % 1000 == 0:\n",
        "  #   print(i)\n",
        "  true_props = data['property'][i]\n",
        "  query = data['q_text'][i].replace('\\n','')\n",
        "  if data['correct'][i]:\n",
        "    query += ' ' + data['option'][i]\n",
        "  else:\n",
        "    query += ' not ' + data['option'][i]\n",
        "\n",
        "  retrieved_props = []\n",
        "  found = False\n",
        "  for cntr in range(i, len(output_queries)):\n",
        "    if query == output_queries[cntr]:\n",
        "      retrieved_props = output_props[cntr]\n",
        "      found = True\n",
        "      # print(i, cntr)\n",
        "      break\n",
        "  if (found == False):\n",
        "    for cntr in range(0,i):\n",
        "      if query == output_queries[cntr]:\n",
        "        retrieved_props = output_props[cntr]\n",
        "        # print(i, cntr)\n",
        "        break\n",
        "  retrieved_props = [x for x in retrieved_props if x]\n",
        "  \n",
        "  num_found = 0\n",
        "  recall0 = 0\n",
        "  precision0 = 0\n",
        "  for prop in retrieved_props:\n",
        "    if prop in true_props:\n",
        "      num_found += 1\n",
        "  precision += float(num_found)/len(retrieved_props)\n",
        "  precision0 = float(num_found)/len(retrieved_props)\n",
        "\n",
        "  num_found = 0\n",
        "  for prop in true_props:\n",
        "    if prop in retrieved_props:\n",
        "      num_found += 1\n",
        "  recall += float(num_found)/len(true_props)\n",
        "  recall0 = float(num_found)/len(true_props)\n",
        "  count += 1\n",
        "  if precision0 + recall0 == 0:\n",
        "    f_score += 0\n",
        "  else:\n",
        "    f_score += 2*precision0*recall0/(precision0 + recall0)\n",
        "\n",
        "precision /= count\n",
        "recall /= count\n",
        "f_score /= count\n",
        "print(\"Exact score============\")\n",
        "print('Recall: ', recall)\n",
        "print('Precision: ', precision)\n",
        "print('F1 Score: ', f_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaZjbG9j_Suy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw0WsbT-_T5j"
      },
      "source": [
        "For evaluation of AIR approach on Silver corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M22owpuQ_X-O"
      },
      "source": [
        "output_file = './MultiRC_BM25_vs_POCC_justification_quality_score/air_omcs_test_output.tsv'\n",
        "test_file = '../data/E2_test.json'\n",
        "name = 'air_omcs_test_output'\n",
        "f = open(test_file)\n",
        "data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vE9u_P5_ey1"
      },
      "source": [
        "output_cols = [\"start\", \"index\", \"zero_zero\", \"correctness\", \"ques\", \"prop0\", \"prop1\", \"prop2\", \"prop3\", \"prop4\", \"prop5\", \"prop6\", \"prop7\",\"prop8\",\"prop9\",\"prop10\",\"prop11\",\"prop12\",\"prop13\",\"prop14\",\"prop15\",\"prop16\",\"prop17\",\"prop18\",\"prop19\",\"prop20\"]\n",
        "df = pd.read_csv(output_file, sep='\\t', names=output_cols, engine='python', header=None)\n",
        "df = df.fillna(\"\")\n",
        "\n",
        "output_queries = df['ques']\n",
        "output_props = []\n",
        "output_correctness = df['correctness']\n",
        "\n",
        "for i,row in df.iterrows():\n",
        "  props = [row['prop0'],row['prop1'],row['prop2'],row['prop3'],row['prop4'],row['prop5'],row['prop6'],row['prop7'],row['prop8'],row['prop9'],row['prop10'],row['prop11'],row['prop12'],row['prop13'],row['prop14'],row['prop15'],row['prop16'],row['prop17'],row['prop18'],row['prop19'],row['prop20']]\n",
        "  output_props.append(props)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sKZI_ig_ff1"
      },
      "source": [
        "%cd ../../generation/cider"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5cB-EwM_ja3"
      },
      "source": [
        "cider_refs_thresholder = []\n",
        "cider_test_thresholder = []\n",
        "spice_thresholder = []\n",
        "count = 0\n",
        "for k in range(0, len(data['q_text'])):\n",
        "  # if k % 500 == 0:\n",
        "  #   print(k)\n",
        "  true_props = data['property'][k]\n",
        "  query = data['q_text'][k].replace('\\n','')\n",
        "  if data['correct'][k]:\n",
        "    query += ' ' + data['option'][k]\n",
        "  else:\n",
        "    query += ' not ' + data['option'][k]\n",
        "\n",
        "  retrieved_props = []\n",
        "  found = False\n",
        "  for cntr in range(k, len(output_queries)):\n",
        "    if query == output_queries[cntr]:\n",
        "      retrieved_props = output_props[cntr]\n",
        "      found = True\n",
        "      if cntr != k:\n",
        "        print(k, cntr)\n",
        "      break\n",
        "  if (found == False):\n",
        "    for cntr in range(0,k):\n",
        "      if query == output_queries[cntr]:\n",
        "        retrieved_props = output_props[cntr]\n",
        "        print(k, cntr)\n",
        "        break\n",
        "  retrieved_props = output_props[cntr]\n",
        "  retrieved_props = [x for x in retrieved_props if x]\n",
        "  correctness = data['correct'][k]\n",
        "\n",
        "  for i in range(len(true_props)):\n",
        "    for j in range(len(retrieved_props)):\n",
        "      struct = {\n",
        "          \"image_id\": count,\n",
        "          \"caption\": retrieved_props[j]\n",
        "      }\n",
        "      cider_test_thresholder.append(struct)\n",
        "      struct = {\n",
        "          \"image_id\": count,\n",
        "          \"caption\": true_props[i]\n",
        "      }\n",
        "      cider_refs_thresholder.append(struct)\n",
        "      struct = {\n",
        "          \"image_id\": count,\n",
        "          \"test\": retrieved_props[j],\n",
        "          \"refs\": [true_props[i]]\n",
        "      }\n",
        "      spice_thresholder.append(struct)\n",
        "      count += 1\n",
        "\n",
        "with open('./data/cider_' + name + '_refs.json', 'w') as outfile:\n",
        "    json.dump(cider_refs_thresholder, outfile)\n",
        "with open('./data/cider_' + name + '_test.json', 'w') as outfile:\n",
        "    json.dump(cider_test_thresholder, outfile)\n",
        "with open('../spice/spice_' + name + '.json', 'w') as outfile:\n",
        "    json.dump(spice_thresholder, outfile)\n",
        "\n",
        "params = {\n",
        "  \"pathToData\" : \"./data/\",\n",
        "    \"refName\" : 'cider_' + name + '_refs.json',\n",
        "    \"candName\" : 'cider_' + name + '_test.json',\n",
        "    \"resultFile\" : 'cider_' + name + '_results.json',\n",
        "    \"idf\" : \"coco-val-df\"\n",
        "}\n",
        "with open('./params.json', 'w') as outfile:\n",
        "    json.dump(params, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfF0038NAGBd"
      },
      "source": [
        "!python2 ./cidereval.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBo6uQW2ALZY"
      },
      "source": [
        "file2 = open('./cider_' + name + '_results.json')\n",
        "cider_output = json.load(file2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5szHlrQjAM0_"
      },
      "source": [
        "%cd ../meteor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9OqJNgcATfo"
      },
      "source": [
        "write_file = open(name + \"_meteor_refs\", \"w\")\n",
        "for i in range(len(cider_refs_thresholder)):\n",
        "  new_line = cider_refs_thresholder[i]['caption'].replace(\"\\n\", \" \") + \" \\n\"\n",
        "  write_file.write(new_line)\n",
        "write_file.close()\n",
        "\n",
        "write_file2 = open(name + \"_meteor_test\", \"w\")\n",
        "for i in range(len(cider_test_thresholder)):\n",
        "  if (cider_test_thresholder[i]['caption'] == \"\" or cider_test_thresholder[i]['caption'] == \" \"):\n",
        "    new_line = \"empty \\n\"\n",
        "  else:\n",
        "    new_line = cider_test_thresholder[i]['caption'].replace(\"\\n\", \" \") + \" \\n\"\n",
        "  write_file2.write(new_line)\n",
        "write_file2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS-L_qpLAVbw"
      },
      "source": [
        "meteor_scores = !java -Xmx2G -jar meteor-1.5.jar ./air_omcs_test_output_meteor_test ./air_omcs_test_output_meteor_refs -l en -norm -a data/paraphrase-en.gz -q\n",
        "meteor_scores = [float(meteor_scores[i]) for i in range(len(meteor_scores))]\n",
        "meteor_scores[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1eCD5klAgkN"
      },
      "source": [
        "%cd ../spice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLK-8Kj4AhC4"
      },
      "source": [
        "!java -Xmx8G -jar spice-1.0.jar spice_air_omcs_test_output.json -cache ./cache/ -out spice_air_omcs_test_output_output.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY5fKK4PAnl7"
      },
      "source": [
        "file2 = open('./spice_' + name + '_output.json')\n",
        "spice_output = json.load(file2)\n",
        "len(spice_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz2WOO3EApqb"
      },
      "source": [
        "rouge_test =  [cider_test_thresholder[i]['caption'] for i in range(len(cider_test_thresholder))]\n",
        "rouge_refs =  [cider_refs_thresholder[i]['caption'] for i in range(len(cider_refs_thresholder))]\n",
        "r = Rouge()\n",
        "rouge_scores = r.compute_score(rouge_refs, rouge_test)\n",
        "rouge_scores[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NLZPjVAArTT"
      },
      "source": [
        "spice_recall = 0.0\n",
        "spice_precision = 0.0\n",
        "spice_fscore = 0.0\n",
        "cider_recall = 0.0\n",
        "cider_precision = 0.0\n",
        "cider_fscore = 0.0\n",
        "meteor_recall = 0.0\n",
        "meteor_precision = 0.0\n",
        "meteor_fscore = 0.0\n",
        "rouge_recall = 0.0\n",
        "rouge_precision = 0.0\n",
        "rouge_fscore = 0.0\n",
        "count1 = 0\n",
        "count = 0\n",
        "spice_threshold = 0.4\n",
        "cider_threshold = 3\n",
        "meteor_threshold = 0.3\n",
        "rouge_threshold = 0.3\n",
        "counter = []\n",
        "for k in range(0, len(data['q_text'])):\n",
        "  # if k % 500 == 0:\n",
        "  #   print(k)\n",
        "  true_props = data['property'][k]\n",
        "  query = data['q_text'][k].replace('\\n','')\n",
        "  if data['correct'][k]:\n",
        "    query += ' ' + data['option'][k]\n",
        "  else:\n",
        "    query += ' not ' + data['option'][k]\n",
        "\n",
        "  retrieved_props = []\n",
        "  found = False\n",
        "  for cntr in range(k, len(output_queries)):\n",
        "    if query == output_queries[cntr]:\n",
        "      retrieved_props = output_props[cntr]\n",
        "      found = True\n",
        "      break\n",
        "  if (found == False):\n",
        "    for cntr in range(0,k):\n",
        "      if query == output_queries[cntr]:\n",
        "        retrieved_props = output_props[cntr]\n",
        "        break\n",
        "\n",
        "  retrieved_props = output_props[cntr]\n",
        "  retrieved_props = [x for x in retrieved_props if x]\n",
        "  correctness = data['correct'][k]\n",
        "  bipartite_graph = np.zeros((len(true_props), len(retrieved_props)))\n",
        "  bipartite_graph_double_spice = np.zeros((len(true_props), len(retrieved_props)))\n",
        "  bipartite_graph_double_meteor = np.zeros((len(true_props), len(retrieved_props)))\n",
        "  bipartite_graph_double_rouge = np.zeros((len(true_props), len(retrieved_props)))\n",
        "  \n",
        "  for i in range(len(true_props)):\n",
        "    for j in range(len(retrieved_props)):\n",
        "      cider_score = cider_output['CIDEr'][count1]\n",
        "      meteor_score = meteor_scores[count1]\n",
        "      rouge_score = rouge_scores[1][count1]\n",
        "      spice_score = spice_output[count1]['scores']['All']['f']\n",
        "      count1 += 1\n",
        "      if (spice_score >= spice_threshold):\n",
        "        bipartite_graph_double_spice[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph_double_spice[i][j] = 0\n",
        "      if (cider_score >= cider_threshold):\n",
        "        bipartite_graph[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph[i][j] = 0\n",
        "      if (meteor_score >= meteor_threshold):\n",
        "        bipartite_graph_double_meteor[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph_double_meteor[i][j] = 0\n",
        "      if (rouge_score >= rouge_threshold):\n",
        "        bipartite_graph_double_rouge[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph_double_rouge[i][j] = 0\n",
        "  \n",
        "  g = GFG(bipartite_graph_double_spice)\n",
        "  number, division_list = g.maxBPM()\n",
        "  \n",
        "  score_recall = 0\n",
        "  score_precision = 0\n",
        "  for i in range(len(true_props)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if (division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "    \n",
        "    if (j != -1):\n",
        "      score_recall += 1\n",
        "      score_precision += 1\n",
        "      count += 1\n",
        "      counter.append(count) \n",
        "    else:\n",
        "      count += 1\n",
        "\n",
        "  spice_recall += score_recall/len(true_props)\n",
        "  spice_precision += score_precision/len(retrieved_props)\n",
        "  a2 = score_recall/len(true_props)\n",
        "  b2 = score_precision/len(retrieved_props)\n",
        "  if (a2+b2 != 0):\n",
        "    spice_fscore += 2*a2*b2/(a2+b2)\n",
        "\n",
        "  g = GFG(bipartite_graph)\n",
        "  number, division_list = g.maxBPM()\n",
        "  \n",
        "  score_recall2 = 0\n",
        "  score_precision2 = 0\n",
        "  for i in range(len(true_props)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if (division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "    \n",
        "    if (j != -1):\n",
        "      score_recall2 += 1\n",
        "      score_precision2 += 1\n",
        "      count += 1\n",
        "      counter.append(count) \n",
        "    else:\n",
        "      count += 1\n",
        "\n",
        "  cider_recall += score_recall2/len(true_props)\n",
        "  cider_precision += score_precision2/len(retrieved_props)\n",
        "  a2 = score_recall2/len(true_props)\n",
        "  b2 = score_precision2/len(retrieved_props)\n",
        "  if (a2+b2 != 0):\n",
        "    cider_fscore += 2*a2*b2/(a2+b2)\n",
        "\n",
        "  g = GFG(bipartite_graph_double_meteor) \n",
        "  number, division_list = g.maxBPM()\n",
        "  \n",
        "  l1 = true_props\n",
        "  l2 = retrieved_props\n",
        "  score_recall3 = 0\n",
        "  score_precision3 = 0\n",
        "  for i in range(len(l1)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if (division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "    if (j != -1):\n",
        "      score_recall3 += 1\n",
        "      score_precision3 += 1\n",
        "      count += 1 \n",
        "    else:\n",
        "      count += 1\n",
        "  meteor_recall += score_recall3/len(l1)\n",
        "  meteor_precision += score_precision3/len(l2)\n",
        "  a2 = score_recall3/len(l1)\n",
        "  b2 = score_precision3/len(l2)\n",
        "  if (a2+b2 != 0):\n",
        "    meteor_fscore += 2*a2*b2/(a2+b2)\n",
        "\n",
        "  g = GFG(bipartite_graph_double_rouge) \n",
        "  number, division_list = g.maxBPM()\n",
        "  \n",
        "  score_recall4 = 0\n",
        "  score_precision4 = 0\n",
        "  for i in range(len(l1)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if (division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "    if (j != -1):\n",
        "      score_recall4 += 1\n",
        "      score_precision4 += 1\n",
        "      count += 1 \n",
        "    else:\n",
        "      count += 1\n",
        "  rouge_recall += score_recall4/len(l1)\n",
        "  rouge_precision += score_precision4/len(l2)\n",
        "  a2 = score_recall4/len(l1)\n",
        "  b2 = score_precision4/len(l2)\n",
        "  if (a2+b2 != 0):\n",
        "    rouge_fscore += 2*a2*b2/(a2+b2)\n",
        "\n",
        "# print(count)\n",
        "# print(len(counter))\n",
        "x = len(data[\"q_text\"])\n",
        "print(\"SPICE==============\")\n",
        "print('Recall: ', spice_recall/x)\n",
        "print('Precision: ', spice_precision/x)\n",
        "print('F1 Score: ', spice_fscore/x)\n",
        "print(\"CIDEr==============\")\n",
        "print('Recall: ', cider_recall/x)\n",
        "print('Precision: ', cider_precision/x)\n",
        "print('F1 Score: ', cider_fscore/x)\n",
        "print(\"METEOR==============\")\n",
        "print(meteor_recall/x)\n",
        "print(meteor_precision/x)\n",
        "print(meteor_fscore/x)\n",
        "print(\"ROUGE==============\")\n",
        "print('Recall: ', rouge_recall/x)\n",
        "print('Precision: ', rouge_precision/x)\n",
        "print('F1 Score: ', rouge_fscore/x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzJV8q1yA_Np"
      },
      "source": [
        "sts_recall = 0.0\n",
        "sts_precision = 0.0\n",
        "sts_fscore = 0.0\n",
        "count = 0\n",
        "sts_threshold = 3\n",
        "\n",
        "counter = []\n",
        "for i in range(0, len(data['q_text'])):\n",
        "  # if i % 500 == 0:\n",
        "  #   print(i)\n",
        "  true_props = data['property'][i]\n",
        "  query = data['q_text'][i].replace('\\n','')\n",
        "  if data['correct'][i]:\n",
        "    query += ' ' + data['option'][i]\n",
        "  else:\n",
        "    query += ' not ' + data['option'][i]\n",
        "\n",
        "  retrieved_props = []\n",
        "  found = False\n",
        "  for cntr in range(i, len(output_queries)):\n",
        "    if query == output_queries[cntr]:\n",
        "      retrieved_props = output_props[cntr]\n",
        "      found = True\n",
        "      break\n",
        "  if (found == False):\n",
        "    for cntr in range(0,i):\n",
        "      if query == output_queries[cntr]:\n",
        "        retrieved_props = output_props[cntr]\n",
        "        break\n",
        "\n",
        "  retrieved_props = [x for x in retrieved_props if x]\n",
        "\n",
        "  bipartite_graph = np.zeros((len(true_props), len(retrieved_props)))\n",
        "  bipartite_graph_double = np.zeros((len(true_props), len(retrieved_props)))\n",
        "  for i in range(len(true_props)):\n",
        "    for j in range(len(retrieved_props)):\n",
        "      sts_score = web_model.predict([(true_props[i], retrieved_props[j])])[0]\n",
        "      bipartite_graph_double[i][j] = sts_score\n",
        "      if sts_score >= sts_threshold:\n",
        "        bipartite_graph[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph[i][j] = 0\n",
        "\n",
        "  g = GFG(bipartite_graph) \n",
        "  number, division_list = g.maxBPM()\n",
        "\n",
        "  score0 = 0\n",
        "  score_recall0 = 0\n",
        "  score_precision0 = 0\n",
        "  for i in range(len(true_props)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if(division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "\n",
        "    if (j != -1):\n",
        "      sts_score = bipartite_graph_double[i][j]\n",
        "      score0 += sts_score\n",
        "      score_recall0 += 1\n",
        "      score_precision0 += 1\n",
        "      count += 1\n",
        "      \n",
        "    else:\n",
        "      count += 1\n",
        "  \n",
        "  sts_recall += score_recall0/len(true_props)\n",
        "  sts_precision += score_precision0/len(retrieved_props)\n",
        "  a0 = score_recall0/len(true_props)\n",
        "  b0 = score_precision0/len(retrieved_props)\n",
        "  if (a0+b0 != 0):\n",
        "    sts_fscore += 2*a0*b0/(a0+b0)\n",
        "\n",
        "x = len(data[\"q_text\"])\n",
        "print(\"STS==============\")\n",
        "print('Recall: ', sts_recall/x)\n",
        "print('Precision: ', sts_precision/x)\n",
        "print('F1 Score: ', sts_fscore/x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5H7OwOSBGHo"
      },
      "source": [
        "%cd ../../retrieval/AIR-retriever"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}