{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "retrieval_eval.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU3LpSo13QuV"
      },
      "source": [
        "!pip install semantic-text-similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvuJ0Z6V40XY"
      },
      "source": [
        "from semantic_text_similarity.models import WebBertSimilarity\n",
        "from semantic_text_similarity.models import ClinicalBertSimilarity\n",
        "\n",
        "web_model = WebBertSimilarity(device='cpu', batch_size=10) #defaults to GPU prediction\n",
        "\n",
        "# clinical_model = ClinicalBertSimilarity(device='cuda', batch_size=10) #defaults to GPU prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7VAgzZm5Uv4"
      },
      "source": [
        "import numpy as np\n",
        "import pdb\n",
        "import json\n",
        "class GFG: \n",
        "  def __init__(self,graph): \n",
        "    self.graph = graph \n",
        "    self.ppl = len(graph) \n",
        "    self.jobs = len(graph[0]) \n",
        "\n",
        "  def bpm(self, u, matchR, seen): \n",
        "    for v in range(self.jobs):  \n",
        "      if self.graph[u][v] and seen[v] == False: \n",
        "        seen[v] = True\n",
        "        if matchR[v] == -1 or self.bpm(matchR[v], matchR, seen): \n",
        "          matchR[v] = u \n",
        "          return True\n",
        "    return False\n",
        "  \n",
        "  # Returns maximum number of matching \n",
        "  def maxBPM(self):\n",
        "    matchR = [-1] * self.jobs \n",
        "\n",
        "    result = 0\n",
        "    for i in range(self.ppl):\n",
        "      seen = [False] * self.jobs\n",
        "      if self.bpm(i, matchR, seen): \n",
        "        result += 1\n",
        "    return result, matchR\n",
        "\n",
        "def my_lcs(string, sub):\n",
        "  if(len(string)<= len(sub)):\n",
        "    sub, string = string, sub\n",
        "\n",
        "    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n",
        "\n",
        "    for j in range(1,len(sub)+1):\n",
        "      for i in range(1,len(string)+1):\n",
        "        if (string[i-1] == sub[j-1]):\n",
        "          lengths[i][j] = lengths[i-1][j-1] + 1\n",
        "        else:\n",
        "          lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n",
        "\n",
        "    return lengths[len(string)][len(sub)]\n",
        "\n",
        "class Rouge():\n",
        "  def __init__(self):\n",
        "    self.beta = 1.2\n",
        "\n",
        "  def calc_score(self, candidate, refs):\n",
        "    assert(len(candidate)==1)\t\n",
        "    assert(len(refs)>0)         \n",
        "    prec = []\n",
        "    rec = []\n",
        "\n",
        "  # split into tokens\n",
        "    token_c = candidate[0].split(\" \")\n",
        "    \t\n",
        "    for reference in refs:\n",
        "      # split into tokens\n",
        "      token_r = reference.split(\" \")\n",
        "      # compute the longest common subsequence\n",
        "      lcs = my_lcs(token_r, token_c)\n",
        "      if (lcs == None):\n",
        "        prec.append(0)\n",
        "        rec.append(0)\n",
        "      else:\n",
        "        prec.append(lcs/float(len(token_c)))\n",
        "        rec.append(lcs/float(len(token_r)))\n",
        "\n",
        "      prec_max = max(prec)\n",
        "      rec_max = max(rec)\n",
        "\n",
        "      if (prec_max!=0 and rec_max !=0):\n",
        "        score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n",
        "      else:\n",
        "        score = 0.0\n",
        "      return score\n",
        "\n",
        "  def compute_score(self, refs, test):\n",
        "    score = []\n",
        "    for i in range(len(refs)):\n",
        "      hypo = test[i]\n",
        "      ref = refs[i]\n",
        "      if (hypo == \" \" or hypo == \"\"):\n",
        "        score.append(0)\n",
        "      else:\n",
        "        score.append(self.calc_score([hypo], [ref]))\n",
        "    \n",
        "    average_score = np.mean(np.array(score))\n",
        "    return average_score, np.array(score)\n",
        "\n",
        "  def method(self):\n",
        "    return \"Rouge\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsZI-Guh5XWM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlnHUqsH5ZwV"
      },
      "source": [
        "For evaluation of Top-k approach on gold corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOWmsnNU41nJ"
      },
      "source": [
        "import json\n",
        "f = open('./3_dx_sets.json')\n",
        "data = json.load(f)\n",
        "name = '3_dx_sets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzuxso4b5Co4"
      },
      "source": [
        "count = 0\n",
        "recall = 0.0\n",
        "precision = 0.0\n",
        "f_score = 0.0\n",
        "for i in range(0, len(data[\"Input\"])):\n",
        "  \n",
        "  true_props = data[\"Gold\"][i]\n",
        "  # retrieved_props = data[\"Output\"][i]\n",
        "  if not data[\"Correctness\"][i]:\n",
        "    retrieved_props = [data[\"Output\"][i][0]]\n",
        "  else:\n",
        "    retrieved_props = data[\"Output\"][i]\n",
        "  num_found = 0\n",
        "  recall0 = 0\n",
        "  precision0 = 0\n",
        "  for prop in retrieved_props:\n",
        "    if prop in true_props:\n",
        "      num_found += 1\n",
        "  precision += float(num_found)/len(retrieved_props)\n",
        "  precision0 = float(num_found)/len(retrieved_props)\n",
        "\n",
        "  num_found = 0\n",
        "  for prop in true_props:\n",
        "    if prop in retrieved_props:\n",
        "      num_found += 1\n",
        "  recall += float(num_found)/len(true_props)\n",
        "  recall0 = float(num_found)/len(true_props)\n",
        "  count += 1\n",
        "  if precision0 + recall0 == 0:\n",
        "    f_score += 0\n",
        "  else:\n",
        "    f_score += 2*precision0*recall0/(precision0 + recall0)\n",
        "\n",
        "precision /= count\n",
        "recall /= count\n",
        "f_score /= count\n",
        "print(\"Exact Score==========\")\n",
        "print('Recall: ', recall)\n",
        "print('Precision: ', precision)\n",
        "print('F1 Score: ', f_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-MfZdVO5Lem"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA-XZNch8mfF"
      },
      "source": [
        "For evaluation of Top-k approach on Silver corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZSm0Hm56is7"
      },
      "source": [
        "import json\n",
        "f = open('./3_omcs_dx_sets.json')\n",
        "data = json.load(f)\n",
        "name = '3_omcs_dx_sets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6elHu4UV6nW9"
      },
      "source": [
        "%cd ../generation/cider"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBvc28Wa6o2I"
      },
      "source": [
        "spice_thresholder = []\n",
        "cider_refs_thresholder = []\n",
        "cider_test_thresholder = []\n",
        "count = 0\n",
        "for k in range(len(data[\"Input\"])):\n",
        "  # if (k % 500 == 0):\n",
        "  #   print(k)\n",
        "  l1 = data[\"Gold\"][k]\n",
        "  # l2 = data[\"Output\"][k]\n",
        "  correctness = data[\"Correctness\"][k]\n",
        "  # top-k policy where k=3 for positive and k=1 for negative\n",
        "  if not correctness:\n",
        "    l2 = [data[\"Output\"][k][0]]\n",
        "  else:\n",
        "    l2 = data[\"Output\"][k]\n",
        "  for i in range(len(l1)):\n",
        "    for j in range(len(l2)):\n",
        "      struct = {\n",
        "          \"image_id\": count,\n",
        "          \"caption\": l2[j]\n",
        "      }\n",
        "      cider_test_thresholder.append(struct)\n",
        "      struct = {\n",
        "          \"image_id\": count,\n",
        "          \"caption\": l1[i]\n",
        "      }\n",
        "      cider_refs_thresholder.append(struct)\n",
        "      struct = {\n",
        "          \"image_id\": count,\n",
        "          \"test\": l2[j],\n",
        "          \"refs\": [l1[i]]\n",
        "      }\n",
        "      spice_thresholder.append(struct)\n",
        "      count += 1\n",
        "\n",
        "with open('./data/cider_' + name + '_refs.json', 'w') as outfile:\n",
        "    json.dump(cider_refs_thresholder, outfile)\n",
        "with open('./data/cider_' + name + '_test.json', 'w') as outfile:\n",
        "    json.dump(cider_test_thresholder, outfile)\n",
        "with open('../spice/spice_' + name + '.json', 'w') as outfile:\n",
        "    json.dump(spice_thresholder, outfile)\n",
        "\n",
        "params = {\n",
        "  \"pathToData\" : \"./data/\",\n",
        "    \"refName\" : 'cider_' + name + '_refs.json',\n",
        "    \"candName\" : 'cider_' + name + '_test.json',\n",
        "    \"resultFile\" : 'cider_' + name + '_results.json',\n",
        "    \"idf\" : \"coco-val-df\"\n",
        "}\n",
        "with open('./params.json', 'w') as outfile:\n",
        "    json.dump(params, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4YaMClb6vx7"
      },
      "source": [
        "!python2 ./cidereval.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoMiyRi360bY"
      },
      "source": [
        "file2 = open('./cider_' + name + '_results.json')\n",
        "cider_output = json.load(file2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyj8Csyb63er"
      },
      "source": [
        "%cd ../meteor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21RfaqzR67E_"
      },
      "source": [
        "write_file = open(name + \"_meteor_refs\", \"w\")\n",
        "for i in range(len(cider_refs_thresholder)):\n",
        "  new_line = cider_refs_thresholder[i]['caption'].replace(\"\\n\", \" \") + \" \\n\"\n",
        "  write_file.write(new_line)\n",
        "write_file.close()\n",
        "\n",
        "write_file2 = open(name + \"_meteor_test\", \"w\")\n",
        "for i in range(len(cider_test_thresholder)):\n",
        "  if (cider_test_thresholder[i]['caption'] == \"\" or cider_test_thresholder[i]['caption'] == \" \"):\n",
        "    new_line = \"empty \\n\"\n",
        "  else:\n",
        "    new_line = cider_test_thresholder[i]['caption'].replace(\"\\n\", \" \") + \" \\n\"\n",
        "  write_file2.write(new_line)\n",
        "write_file2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSnJHRKJ6_Op"
      },
      "source": [
        "meteor_scores = !java -Xmx2G -jar meteor-1.5.jar ./3_omcs_dx_sets_meteor_test ./3_omcs_dx_sets_meteor_refs -l en -norm -a data/paraphrase-en.gz -q\n",
        "meteor_scores = [float(meteor_scores[i]) for i in range(len(meteor_scores))]\n",
        "meteor_scores[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYyUmfRB7ciH"
      },
      "source": [
        "%cd ../spice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1dajISQ7eNs"
      },
      "source": [
        "!java -Xmx8G -jar spice-1.0.jar spice_3_omcs_dx_sets.json -cache ./cache/ -out spice_3_omcs_dx_sets_output.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "557OVmWH7lda"
      },
      "source": [
        "file2 = open('./spice_' + name + '_output.json')\n",
        "spice_output = json.load(file2)\n",
        "len(spice_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46M3VSTT7wQq"
      },
      "source": [
        "rouge_test =  [cider_test_thresholder[i]['caption'] for i in range(len(cider_test_thresholder))]\n",
        "rouge_refs =  [cider_refs_thresholder[i]['caption'] for i in range(len(cider_refs_thresholder))]\n",
        "r = Rouge()\n",
        "rouge_scores = r.compute_score(rouge_refs, rouge_test)\n",
        "rouge_scores[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfHBfDMu7ygL"
      },
      "source": [
        "spice_recall = 0.0\n",
        "spice_precision = 0.0\n",
        "spice_fscore = 0.0\n",
        "cider_recall = 0.0\n",
        "cider_precision = 0.0\n",
        "cider_fscore = 0.0\n",
        "meteor_recall = 0.0\n",
        "meteor_precision = 0.0\n",
        "meteor_fscore = 0.0\n",
        "rouge_recall = 0.0\n",
        "rouge_precision = 0.0\n",
        "rouge_fscore = 0.0\n",
        "count1 = 0\n",
        "count = 0\n",
        "spice_threshold = 0.4\n",
        "cider_threshold = 3\n",
        "meteor_threshold = 0.3\n",
        "rouge_threshold = 0.3\n",
        "counter = []\n",
        "for k in range(len(data[\"Input\"])):\n",
        "  #   if (k % 500 == 0):\n",
        "  #     print(k)\n",
        "  l1 = data[\"Gold\"][k]\t\t\n",
        "  # l2 = data[\"Output\"][k]\n",
        "  correctness = data[\"Correctness\"][k]\n",
        "  # top-k policy where k=3 for positive and k=1 for negative\n",
        "  if not correctness:\n",
        "    l2 = [data[\"Output\"][k][0]]\n",
        "  else:\n",
        "    l2 = data[\"Output\"][k]\n",
        "  \n",
        "  bipartite_graph = np.zeros((len(l1), len(l2)))\n",
        "  bipartite_graph_double_spice = np.zeros((len(l1), len(l2)))\n",
        "  # bipartite_graph_double_meteor = np.zeros((len(l1), len(l2)))\n",
        "  bipartite_graph_double_rouge = np.zeros((len(l1), len(l2)))\n",
        "  \n",
        "  for i in range(len(l1)):\n",
        "    for j in range(len(l2)):\n",
        "      cider_score = cider_output['CIDEr'][count1]\n",
        "      meteor_score = meteor_scores[count1]\n",
        "      rouge_score = rouge_scores[1][count1]\n",
        "      spice_score = spice_output[count1]['scores']['All']['f']\n",
        "      count1 += 1\n",
        "      if (spice_score >= spice_threshold):\n",
        "        bipartite_graph_double_spice[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph_double_spice[i][j] = 0\n",
        "      if (cider_score >= cider_threshold):\n",
        "        bipartite_graph[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph[i][j] = 0\n",
        "      if (meteor_score >= meteor_threshold):\n",
        "        bipartite_graph_double_meteor[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph_double_meteor[i][j] = 0\n",
        "      if (rouge_score >= rouge_threshold):\n",
        "        bipartite_graph_double_rouge[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph_double_rouge[i][j] = 0\n",
        "\n",
        "  g = GFG(bipartite_graph_double_spice)\n",
        "  number, division_list = g.maxBPM()\n",
        "  \n",
        "  score_recall1 = 0\n",
        "  score_precision1 = 0\n",
        "  for i in range(len(l1)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if (division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "    \n",
        "    if (j != -1):\n",
        "      score_recall1 += 1\n",
        "      score_precision1 += 1\n",
        "      count += 1\n",
        "    else:\n",
        "      count += 1\n",
        "\n",
        "  spice_recall += score_recall1/len(l1)\n",
        "  spice_precision += score_precision1/len(l2)\n",
        "  a1 = score_recall1/len(l1)\n",
        "  b1 = score_precision1/len(l2)\n",
        "  if (a1+b1 != 0):\n",
        "    spice_fscore += 2*a1*b1/(a1+b1)\n",
        "\n",
        "  g = GFG(bipartite_graph)\n",
        "  number, division_list = g.maxBPM()\n",
        "  \n",
        "  score_recall2 = 0\n",
        "  score_precision2 = 0\n",
        "  for i in range(len(l1)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if (division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "    \n",
        "    if (j != -1):\n",
        "      score_recall2 += 1\n",
        "      score_precision2 += 1\n",
        "      count += 1 \n",
        "    else:\n",
        "      count += 1\n",
        "\n",
        "  cider_recall += score_recall2/len(l1)\n",
        "  cider_precision += score_precision2/len(l2)\n",
        "  a2 = score_recall2/len(l1)\n",
        "  b2 = score_precision2/len(l2)\n",
        "  if (a2+b2 != 0):\n",
        "    cider_fscore += 2*a2*b2/(a2+b2)\n",
        "\n",
        "  g = GFG(bipartite_graph_double_meteor) \n",
        "  number, division_list = g.maxBPM()\n",
        "  \n",
        "  score_recall3 = 0\n",
        "  score_precision3 = 0\n",
        "  for i in range(len(l1)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if (division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "    if (j != -1):\n",
        "      score_recall3 += 1\n",
        "      score_precision3 += 1\n",
        "      count += 1 \n",
        "    else:\n",
        "      count += 1\n",
        "  meteor_recall += score_recall3/len(l1)\n",
        "  meteor_precision += score_precision3/len(l2)\n",
        "  a2 = score_recall3/len(l1)\n",
        "  b2 = score_precision3/len(l2)\n",
        "  if (a2+b2 != 0):\n",
        "    meteor_fscore += 2*a2*b2/(a2+b2)\n",
        "\n",
        "  g = GFG(bipartite_graph_double_rouge) \n",
        "  number, division_list = g.maxBPM()\n",
        "  \n",
        "  score_recall4 = 0\n",
        "  score_precision4 = 0\n",
        "  for i in range(len(l1)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if (division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "    if (j != -1):\n",
        "      score_recall4 += 1\n",
        "      score_precision4 += 1\n",
        "      count += 1 \n",
        "    else:\n",
        "      count += 1\n",
        "  rouge_recall += score_recall4/len(l1)\n",
        "  rouge_precision += score_precision4/len(l2)\n",
        "  a2 = score_recall4/len(l1)\n",
        "  b2 = score_precision4/len(l2)\n",
        "  if (a2+b2 != 0):\n",
        "    rouge_fscore += 2*a2*b2/(a2+b2)\n",
        "\n",
        "x = len(data[\"Input\"])\n",
        "print(\"SPICE==============\")\n",
        "print('Recall: ', spice_recall/x)\n",
        "print('Precision: ', spice_precision/x)\n",
        "print('F1 Score: ', spice_fscore/x)\n",
        "print(\"CIDEr==============\")\n",
        "print('Recall: ', cider_recall/x)\n",
        "print('Precision: ', cider_precision/x)\n",
        "print('F1 Score: ', cider_fscore/x)\n",
        "print(\"METEOR==============\")\n",
        "print(meteor_recall/x)\n",
        "print(meteor_precision/x)\n",
        "print(meteor_fscore/x)\n",
        "print(\"ROUGE==============\")\n",
        "print('Recall: ', rouge_recall/x)\n",
        "print('Precision: ', rouge_precision/x)\n",
        "print('F1 Score: ', rouge_fscore/x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtXm-hm08r8Q"
      },
      "source": [
        "sts_recall = 0.0\n",
        "sts_precision = 0.0\n",
        "sts_fscore = 0.0\n",
        "count = 0\n",
        "sts_threshold = 3\n",
        "\n",
        "counter = []\n",
        "for k in range(len(data[\"Input\"])):\n",
        "#   if (k % 500 == 0):\n",
        "#     print(k)\n",
        "  l1 = data[\"Gold\"][k]\n",
        "  # l2 = data[\"GPT2_Output\"][k]\n",
        "  if data[\"Correctness\"][k]:\n",
        "    l2 = data[\"Output\"][k]\n",
        "  else:\n",
        "    l2 = [data[\"Output\"][k][0]]\n",
        "  bipartite_graph = np.zeros((len(l1), len(l2)))\n",
        "  bipartite_graph_double = np.zeros((len(l1), len(l2)))\n",
        "  \n",
        "  for i in range(len(l1)):\n",
        "    for j in range(len(l2)):\n",
        "      sts_score = web_model.predict([(l1[i], l2[j])])[0]\n",
        "      bipartite_graph_double[i][j] = sts_score\n",
        "      if (sts_score >= sts_threshold):\n",
        "        bipartite_graph[i][j] = 1\n",
        "      else:\n",
        "        bipartite_graph[i][j] = 0\n",
        "\n",
        "  # g = GFG(bipartite_graph_double) \n",
        "  # number, division_list = g.maxBPM()\n",
        "\n",
        "  # # property i will be matched with division_list[i]  \n",
        "  # score0 = 0\n",
        "  # for i in range(len(l1)):\n",
        "  #   j = -1\n",
        "  #   for k in range(len(division_list)):\n",
        "  #     if(division_list[k] == i):\n",
        "  #       j = k\n",
        "  #       break\n",
        "\n",
        "  #   if (j != -1):\n",
        "  #     sts_score = bipartite_graph_double[i][j]\n",
        "  #     score0 += sts_score\n",
        "  #     count += 1\n",
        "  #     # sts_predictions_array2.append(sts_score)\n",
        "      \n",
        "  #   else:\n",
        "  #     count += 1\n",
        "  #     # sts_predictions_array2.append(0)\n",
        "\n",
        "  # # sts_predictions_array.append(score0/len(l1))\n",
        "\n",
        "  g = GFG(bipartite_graph) \n",
        "  number, division_list = g.maxBPM()\n",
        "\n",
        "  # property i will be matched with division_list[i]  \n",
        "  score_recall0 = 0\n",
        "  score_precision0 = 0\n",
        "  for i in range(len(l1)):\n",
        "    j = -1\n",
        "    for k in range(len(division_list)):\n",
        "      if(division_list[k] == i):\n",
        "        j = k\n",
        "        break\n",
        "\n",
        "    if (j != -1):\n",
        "      score_recall0 += 1\n",
        "      score_precision0 += 1\n",
        "      count += 1\n",
        "    else:\n",
        "      count += 1\n",
        "  sts_recall += score_recall0/len(l1)\n",
        "  sts_precision += score_precision0/len(l2)\n",
        "  a0 = score_recall0/len(l1)\n",
        "  b0 = score_precision0/len(l2)\n",
        "  if (a0+b0 != 0):\n",
        "    sts_fscore += 2*a0*b0/(a0+b0)\n",
        "\n",
        "x = len(data[\"Input\"])\n",
        "print(\"STS Score==============\")\n",
        "print('Recall: ', sts_recall/x)\n",
        "print('Precision: ', sts_precision/x)\n",
        "print('F1 Score: ', sts_fscore/x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-lWhO_d8sTJ"
      },
      "source": [
        "%cd ../../retrieval"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}